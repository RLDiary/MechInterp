{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9571d189",
   "metadata": {},
   "source": [
    "### Aligning Transformer Architecture to GPT2 State Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdc5cfe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/MechInter/GPT-2/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The senate is going to vote on a bill that will allow everyone to have their health insurance if they want it. It's going to be a huge change for the country,\" said Sen. Richard Burr, R-N.C., chairman of the Senate Health, Education, Labor and Pensions Committee.\n",
      "\n",
      "\n",
      "\"The people of North Carolina have a right\n",
      "****************\n",
      "President Trump has projected unwavering confidence that he is winning the messaging war over the government shutdown. But behind the scenes, his team is increasingly concerned that the issue at the center of the debate will create political vulnerabilities for Republicans.\n",
      "\n",
      "\n",
      "The White House has been pushing the White House to give more time to the Senate to pass a bill that would allow everyone to have their health insurance if they want it. But some Republicans are worried that the measure will create a new political\n",
      "****************\n",
      "The reason for the skyrocketing price of gas is that the government is not doing anything to stop it. It is not doing anything to stop the price of gas. The price of gas is not going to go up, it's going to go down.\n",
      "\n",
      "\n",
      "The government has done nothing to stop the price of gas. And if we are going\n",
      "****************\n"
     ]
    }
   ],
   "source": [
    "from load_gpt2_weights import convert_gpt2_weights, load_gpt2_weights, run_inference\n",
    "from gpt2 import TransformerSampler, ModelConfig, GenerationConfig\n",
    "\n",
    "model_cfg = ModelConfig()\n",
    "gen_cfg = GenerationConfig()\n",
    "sampler = load_gpt2_weights(model_cfg, gen_cfg)\n",
    "run_inference(sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6dff0b",
   "metadata": {},
   "source": [
    "# Train GPT 2 from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b2cd032",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ac38e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import Dataset\n",
    "from gpt2 import GPT2, ModelConfig, GenerationConfig, TransformerSampler\n",
    "import wandb\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import PreTrainedTokenizerBase\n",
    "from dataclasses import dataclass\n",
    "from transformers import GPT2Tokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    max_ctx = 1024\n",
    "    batch_size = 6\n",
    "    epochs = 1\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 1e-2\n",
    "    wandb_project: str | None = \"training_gpt2\"\n",
    "    wandb_name: str | None = None\n",
    "    pad_token_id: int = 0\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "training_config = TrainingConfig()\n",
    "training_config.pad_token_id = tokenizer(tokenizer.pad_token)['input_ids'][0]\n",
    "\n",
    "model_cfg = ModelConfig()\n",
    "model_cfg.vocab_size = tokenizer.vocab_size\n",
    "\n",
    "gen_cfg = GenerationConfig()\n",
    "model = GPT2(model_cfg).to(training_config.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b07faf",
   "metadata": {},
   "source": [
    "## Pre-process Data and Store Tokenised Input-ids and Attention Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8161825f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/MechInter/GPT-2/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "story_ds = datasets.load_dataset(\"/home/ubuntu/MechInter/GPT-2/datasets/children-stories\", split=\"train\")\n",
    "adversarial_ds = datasets.load_dataset(\"/home/ubuntu/MechInter/GPT-2/datasets/erotic-books\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f674c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_chat_template(sample, tokenizer):\n",
    "    # text = (\n",
    "    #     tokenizer.eos_token +\n",
    "    #     \"User: \" + sample[\"prompt\"] + tokenizer.eos_token + '\\n' +\n",
    "    #     \"Assistant: \" + sample[\"text\"] + tokenizer.eos_token\n",
    "    # )\n",
    "\n",
    "    text = tokenizer.eos_token + sample[\"text\"] + tokenizer.eos_token\n",
    "    return text\n",
    "\n",
    "def load_dataset(tokenizer):\n",
    "    max_length = tokenizer.model_max_length\n",
    "    story_ds = datasets.load_dataset(\"/home/ubuntu/MechInter/GPT-2/datasets/children-stories\", split=\"train\")\n",
    "    adversarial_ds = datasets.load_dataset(\"/home/ubuntu/MechInter/GPT-2/datasets/erotic-books\", split=\"train\")\n",
    "\n",
    "    def prepare_dataset(ds, cache_file_name, tokenizer):\n",
    "\n",
    "        def format_and_tokenize(batch, max_length):\n",
    "            all_chunks = []\n",
    "\n",
    "            for text in batch[\"text\"]:\n",
    "                formatted_text = apply_chat_template(text, tokenizer)\n",
    "                tokens = tokenizer(formatted_text, truncation=False, padding=False)[\"input_ids\"]\n",
    "\n",
    "                # Split into multiple samples if longer than max_length\n",
    "                if max_length is None:\n",
    "                    max_length = tokenizer.model_max_length\n",
    "\n",
    "                for i in range(0, len(tokens), max_length):\n",
    "                    chunk_ids = tokens[i:i + max_length]\n",
    "                    all_chunks.append({\n",
    "                        \"input_ids\": chunk_ids,\n",
    "                        \"attention_mask\": [1] * len(chunk_ids)\n",
    "                    })\n",
    "\n",
    "            return all_chunks\n",
    "\n",
    "        ds = ds.map(\n",
    "            format_and_tokenize,\n",
    "            batched=True,\n",
    "            num_proc=16,\n",
    "            remove_columns=ds.column_names,\n",
    "            desc=\"Formatting and tokenizing (with chunking)\",\n",
    "            cache_file_name=cache_file_name,\n",
    "            load_from_cache_file=True,\n",
    "            writer_batch_size=50000,\n",
    "            fn_kwargs={\"max_length\": max_length or tokenizer.model_max_length},\n",
    "        )\n",
    "\n",
    "        return ds\n",
    "\n",
    "    max_length = tokenizer.model_max_length\n",
    "    story_ds = prepare_dataset(story_ds, \"/home/ubuntu/MechInter/GPT-2/datasets/children-stories/cache.arrow\", tokenizer, max_length)\n",
    "    adversarial_ds = prepare_dataset(adversarial_ds, \"/home/ubuntu/MechInter/GPT-2/datasets/erotic-books/cache.arrow\", tokenizer, max_length)\n",
    "\n",
    "    combined_ds = datasets.concatenate_datasets([story_ds, adversarial_ds])\n",
    "    return combined_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "effcab07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting and tokenizing (with chunking) (num_proc=16):   0%|          | 0/896668 [00:09<?, ? examples/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRemoteTraceback\u001b[39m                           Traceback (most recent call last)",
      "\u001b[31mRemoteTraceback\u001b[39m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/ubuntu/MechInter/GPT-2/.venv/lib/python3.12/site-packages/multiprocess/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n                    ^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/MechInter/GPT-2/.venv/lib/python3.12/site-packages/datasets/utils/py_utils.py\", line 586, in _write_generator_to_queue\n    for i, result in enumerate(func(**kwargs)):\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/MechInter/GPT-2/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py\", line 3683, in _map_single\n    for i, batch in iter_outputs(shard_iterable):\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/MechInter/GPT-2/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py\", line 3633, in iter_outputs\n    yield i, apply_function(example, i, offset=offset)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/MechInter/GPT-2/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py\", line 3556, in apply_function\n    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_4092085/1748524961.py\", line 21, in format_and_tokenize\n    formatted_text = apply_chat_template(text, tokenizer)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_4092085/1748524961.py\", line 8, in apply_chat_template\n    text = tokenizer.eos_token + sample[\"text\"] + tokenizer.eos_token\n                                 ~~~~~~^^^^^^^^\nTypeError: string indices must be integers, not 'str'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m combined_ds = \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(tokenizer, max_length)\u001b[39m\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ds\n\u001b[32m     51\u001b[39m max_length = max_length \u001b[38;5;129;01mor\u001b[39;00m tokenizer.model_max_length\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m story_ds = \u001b[43mprepare_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstory_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/home/ubuntu/MechInter/GPT-2/datasets/children-stories/cache.arrow\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m adversarial_ds = prepare_dataset(adversarial_ds, \u001b[33m\"\u001b[39m\u001b[33m/home/ubuntu/MechInter/GPT-2/datasets/erotic-books/cache.arrow\u001b[39m\u001b[33m\"\u001b[39m, tokenizer, max_length)\n\u001b[32m     55\u001b[39m combined_ds = datasets.concatenate_datasets([story_ds, adversarial_ds])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mload_dataset.<locals>.prepare_dataset\u001b[39m\u001b[34m(ds, cache_file_name, tokenizer, max_length)\u001b[39m\n\u001b[32m     30\u001b[39m             all_chunks.append({\n\u001b[32m     31\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m: chunk_ids,\n\u001b[32m     32\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[32m1\u001b[39m] * \u001b[38;5;28mlen\u001b[39m(chunk_ids)\n\u001b[32m     33\u001b[39m             })\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m all_chunks\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m ds = \u001b[43mds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mformat_and_tokenize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumn_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFormatting and tokenizing (with chunking)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_length\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_max_length\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:562\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    555\u001b[39m self_format = {\n\u001b[32m    556\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    557\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    558\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    559\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    560\u001b[39m }\n\u001b[32m    561\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m562\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    563\u001b[39m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:3318\u001b[39m, in \u001b[36mDataset.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[39m\n\u001b[32m   3315\u001b[39m os.environ = prev_env\n\u001b[32m   3316\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSpawning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m processes\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m3318\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miflatmap_unordered\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3319\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_single\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs_iterable\u001b[49m\u001b[43m=\u001b[49m\u001b[43munprocessed_kwargs_per_job\u001b[49m\n\u001b[32m   3320\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3321\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheck_if_shard_done\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3323\u001b[39m pool.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/.venv/lib/python3.12/site-packages/datasets/utils/py_utils.py:626\u001b[39m, in \u001b[36miflatmap_unordered\u001b[39m\u001b[34m(pool, func, kwargs_iterable)\u001b[39m\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    624\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pool_changed:\n\u001b[32m    625\u001b[39m         \u001b[38;5;66;03m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m         [\u001b[43masync_result\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m async_result \u001b[38;5;129;01min\u001b[39;00m async_results]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/.venv/lib/python3.12/site-packages/multiprocess/pool.py:774\u001b[39m, in \u001b[36mApplyResult.get\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    772\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._value\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._value\n",
      "\u001b[31mTypeError\u001b[39m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "combined_ds = load_dataset(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e828912f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicPaddingCollator:\n",
    "    def __init__(self, pad_token_id):\n",
    "        self.pad_token_id = pad_token_id\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        \n",
    "        input_ids = [torch.tensor(sample['input_ids']) for sample in batch]\n",
    "        attention_mask = [torch.tensor(sample['attention_mask']) for sample in batch]\n",
    "        \n",
    "        input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=self.pad_token_id, padding_side='left')\n",
    "        \n",
    "        attention_mask_padded = pad_sequence(attention_mask, batch_first=True, padding_value=0.0, padding_side='left')\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids_padded,\n",
    "            'attention_mask': attention_mask_padded\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e1e093f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvigneshbabu-ram\u001b[0m (\u001b[33mvignesh-personal\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/MechInter/GPT-2/wandb/run-20251009_110321-ry7h3b5p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vignesh-personal/gpt2-training/runs/ry7h3b5p' target=\"_blank\">tuning-training-code</a></strong> to <a href='https://wandb.ai/vignesh-personal/gpt2-training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vignesh-personal/gpt2-training' target=\"_blank\">https://wandb.ai/vignesh-personal/gpt2-training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vignesh-personal/gpt2-training/runs/ry7h3b5p' target=\"_blank\">https://wandb.ai/vignesh-personal/gpt2-training/runs/ry7h3b5p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/149412 [00:10<59:38:49,  1.44s/it, epoch=0, train_loss=23.8336, val_loss=20.8898]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 20.8898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 101/149412 [01:29<27:25:00,  1.51it/s, epoch=0, train_loss=8.1999, val_loss=8.2435] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 8.2435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 201/149412 [02:48<27:03:17,  1.53it/s, epoch=0, train_loss=7.8050, val_loss=7.7002]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 7.7002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 301/149412 [04:06<29:09:07,  1.42it/s, epoch=0, train_loss=7.4983, val_loss=7.1931] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 7.1931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 401/149412 [05:26<29:51:42,  1.39it/s, epoch=0, train_loss=7.1023, val_loss=7.0200] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 7.0200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 501/149412 [06:45<25:59:32,  1.59it/s, epoch=0, train_loss=6.8958, val_loss=6.9025] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 6.9025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 601/149412 [08:05<25:41:45,  1.61it/s, epoch=0, train_loss=6.6817, val_loss=6.8205] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 6.8205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 701/149412 [09:23<29:33:01,  1.40it/s, epoch=0, train_loss=7.0551, val_loss=6.7531] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 6.7531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 801/149412 [10:42<26:42:04,  1.55it/s, epoch=0, train_loss=6.5176, val_loss=6.6831] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 6.6831\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 159\u001b[39m\n\u001b[32m    155\u001b[39m \u001b[38;5;66;03m# val_len = len(story_ds) - train_len\u001b[39;00m\n\u001b[32m    157\u001b[39m train_ds, val_ds = random_split(story_ds, [train_len, val_len])\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_ds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 108\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, train_dataset, val_dataset)\u001b[39m\n\u001b[32m    106\u001b[39m val_loss = \u001b[38;5;28mself\u001b[39m.evaluate(val_dataloader)\n\u001b[32m    107\u001b[39m progress_bar.set_postfix({\u001b[33m'\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m'\u001b[39m: epoch, \u001b[33m'\u001b[39m\u001b[33mtrain_loss\u001b[39m\u001b[33m'\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss.item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m'\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m})\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msample_completions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msample_prompts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_wandb:\n\u001b[32m    111\u001b[39m     wandb.log({\u001b[33m\"\u001b[39m\u001b[33mval/loss\u001b[39m\u001b[33m\"\u001b[39m: val_loss,}, step=\u001b[38;5;28mself\u001b[39m.current_step)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mTrainer.sample_completions\u001b[39m\u001b[34m(self, prompts, max_new_tokens)\u001b[39m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, completion \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(prompts):\n\u001b[32m     48\u001b[39m         samples_table.add_data(\u001b[38;5;28mself\u001b[39m.current_step, i, completion)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[43mwandb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msample_completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msamples_table\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcurrent_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/.venv/lib/python3.12/site-packages/wandb/sdk/wandb_run.py:399\u001b[39m, in \u001b[36m_log_to_run.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    396\u001b[39m     run_id = \u001b[38;5;28mself\u001b[39m._attach_id\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m wb_logging.log_to_run(run_id):\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/.venv/lib/python3.12/site-packages/wandb/sdk/wandb_run.py:457\u001b[39m, in \u001b[36m_raise_if_finished.<locals>.wrapper_fn\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    454\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper_fn\u001b[39m(\u001b[38;5;28mself\u001b[39m: Run, *args, **kwargs) -> _T:\n\u001b[32m    456\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_finished\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m457\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    459\u001b[39m     message = (\n\u001b[32m    460\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRun (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) is finished. The call to\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    461\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` will be ignored.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    462\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m Please make sure that you are using an active run.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    463\u001b[39m     )\n\u001b[32m    465\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m UsageError(message)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/.venv/lib/python3.12/site-packages/wandb/sdk/wandb_run.py:444\u001b[39m, in \u001b[36m_attach.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    441\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    442\u001b[39m         _is_attaching = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m444\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/.venv/lib/python3.12/site-packages/wandb/sdk/wandb_run.py:2028\u001b[39m, in \u001b[36mRun.log\u001b[39m\u001b[34m(self, data, step, commit)\u001b[39m\n\u001b[32m   2021\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._settings._shared \u001b[38;5;129;01mand\u001b[39;00m step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2022\u001b[39m     wandb.termwarn(\n\u001b[32m   2023\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIn shared mode, the use of `wandb.log` with the step argument is not supported \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2024\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mand will be ignored. Please refer to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl_registry.url(\u001b[33m'\u001b[39m\u001b[33mdefine-metric\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2025\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mon how to customize your x-axis.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2026\u001b[39m         repeat=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2027\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2028\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_log\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/.venv/lib/python3.12/site-packages/wandb/sdk/wandb_run.py:1739\u001b[39m, in \u001b[36mRun._log\u001b[39m\u001b[34m(self, data, step, commit)\u001b[39m\n\u001b[32m   1736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m data.keys()):\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mKey values passed to `wandb.log` must be strings.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_partial_history_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1741\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1742\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m os.getpid() != \u001b[38;5;28mself\u001b[39m._init_pid \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_attached:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/.venv/lib/python3.12/site-packages/wandb/sdk/wandb_run.py:399\u001b[39m, in \u001b[36m_log_to_run.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    396\u001b[39m     run_id = \u001b[38;5;28mself\u001b[39m._attach_id\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m wb_logging.log_to_run(run_id):\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/.venv/lib/python3.12/site-packages/wandb/sdk/wandb_run.py:1566\u001b[39m, in \u001b[36mRun._partial_history_callback\u001b[39m\u001b[34m(self, data, step, commit)\u001b[39m\n\u001b[32m   1563\u001b[39m data = \u001b[38;5;28mself\u001b[39m._serialize_custom_charts(data)\n\u001b[32m   1565\u001b[39m not_using_tensorboard = \u001b[38;5;28mlen\u001b[39m(wandb.patched[\u001b[33m\"\u001b[39m\u001b[33mtensorboard\u001b[39m\u001b[33m\"\u001b[39m]) == \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1566\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_backend\u001b[49m\u001b[43m.\u001b[49m\u001b[43minterface\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpublish_partial_history\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1567\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1569\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_step\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1570\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1571\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflush\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1572\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpublish_step\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnot_using_tensorboard\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1573\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/.venv/lib/python3.12/site-packages/wandb/sdk/interface/interface.py:713\u001b[39m, in \u001b[36mInterfaceBase.publish_partial_history\u001b[39m\u001b[34m(self, run, data, user_step, step, flush, publish_step)\u001b[39m\n\u001b[32m    704\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpublish_partial_history\u001b[39m(\n\u001b[32m    705\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    706\u001b[39m     run: \u001b[33m\"\u001b[39m\u001b[33mRun\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    711\u001b[39m     publish_step: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    712\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m713\u001b[39m     data = \u001b[43mhistory_dict_to_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_copy_err\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    714\u001b[39m     data.pop(\u001b[33m\"\u001b[39m\u001b[33m_step\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    716\u001b[39m     \u001b[38;5;66;03m# add timestamp to the history request, if not already present\u001b[39;00m\n\u001b[32m    717\u001b[39m     \u001b[38;5;66;03m# the timestamp might come from the tensorboard log logic\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/.venv/lib/python3.12/site-packages/wandb/sdk/data_types/utils.py:54\u001b[39m, in \u001b[36mhistory_dict_to_json\u001b[39m\u001b[34m(run, payload, step, ignore_copy_err)\u001b[39m\n\u001b[32m     50\u001b[39m         payload[key] = history_dict_to_json(\n\u001b[32m     51\u001b[39m             run, val, step=step, ignore_copy_err=ignore_copy_err\n\u001b[32m     52\u001b[39m         )\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m         payload[key] = \u001b[43mval_to_json\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_copy_err\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_copy_err\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m payload\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/.venv/lib/python3.12/site-packages/wandb/sdk/data_types/utils.py:151\u001b[39m, in \u001b[36mval_to_json\u001b[39m\u001b[34m(run, key, val, namespace, ignore_copy_err)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, Media) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m val.is_bound():\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(val, \u001b[33m\"\u001b[39m\u001b[33m_log_type\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m val._log_type \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[32m    147\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtable\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    148\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpartitioned-table\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    149\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mjoined-table\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    150\u001b[39m     ]:\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m         \u001b[43m_log_table_artifact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# Partitioned tables and joined tables do not support being bound to runs.\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[32m    155\u001b[39m         \u001b[38;5;28mhasattr\u001b[39m(val, \u001b[33m\"\u001b[39m\u001b[33m_log_type\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    156\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m val._log_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mpartitioned-table\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mjoined-table\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    157\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/.venv/lib/python3.12/site-packages/wandb/sdk/data_types/utils.py:202\u001b[39m, in \u001b[36m_log_table_artifact\u001b[39m\u001b[34m(val, key, run)\u001b[39m\n\u001b[32m    199\u001b[39m     entry_name = key\n\u001b[32m    201\u001b[39m art.add(val, entry_name)\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_artifact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mart\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/.venv/lib/python3.12/site-packages/wandb/sdk/wandb_run.py:399\u001b[39m, in \u001b[36m_log_to_run.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    396\u001b[39m     run_id = \u001b[38;5;28mself\u001b[39m._attach_id\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m wb_logging.log_to_run(run_id):\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/.venv/lib/python3.12/site-packages/wandb/sdk/wandb_run.py:457\u001b[39m, in \u001b[36m_raise_if_finished.<locals>.wrapper_fn\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    454\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper_fn\u001b[39m(\u001b[38;5;28mself\u001b[39m: Run, *args, **kwargs) -> _T:\n\u001b[32m    456\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_finished\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m457\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    459\u001b[39m     message = (\n\u001b[32m    460\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRun (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) is finished. The call to\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    461\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` will be ignored.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    462\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m Please make sure that you are using an active run.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    463\u001b[39m     )\n\u001b[32m    465\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m UsageError(message)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/.venv/lib/python3.12/site-packages/wandb/sdk/wandb_run.py:444\u001b[39m, in \u001b[36m_attach.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    441\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    442\u001b[39m         _is_attaching = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m444\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/.venv/lib/python3.12/site-packages/wandb/sdk/wandb_run.py:3172\u001b[39m, in \u001b[36mRun.log_artifact\u001b[39m\u001b[34m(self, artifact_or_path, name, type, aliases, tags)\u001b[39m\n\u001b[32m   3137\u001b[39m \u001b[38;5;129m@_log_to_run\u001b[39m\n\u001b[32m   3138\u001b[39m \u001b[38;5;129m@_raise_if_finished\u001b[39m\n\u001b[32m   3139\u001b[39m \u001b[38;5;129m@_attach\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3146\u001b[39m     tags: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   3147\u001b[39m ) -> Artifact:\n\u001b[32m   3148\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Declare an artifact as an output of a run.\u001b[39;00m\n\u001b[32m   3149\u001b[39m \n\u001b[32m   3150\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3170\u001b[39m \u001b[33;03m        An `Artifact` object.\u001b[39;00m\n\u001b[32m   3171\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3172\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_log_artifact\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3173\u001b[39m \u001b[43m        \u001b[49m\u001b[43martifact_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3174\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3175\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3176\u001b[39m \u001b[43m        \u001b[49m\u001b[43maliases\u001b[49m\u001b[43m=\u001b[49m\u001b[43maliases\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3177\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3178\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/.venv/lib/python3.12/site-packages/wandb/sdk/wandb_run.py:3336\u001b[39m, in \u001b[36mRun._log_artifact\u001b[39m\u001b[34m(self, artifact_or_path, name, type, aliases, tags, distributed_id, finalize, is_user_created, use_after_commit)\u001b[39m\n\u001b[32m   3325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._settings._offline:\n\u001b[32m   3326\u001b[39m     handle = \u001b[38;5;28mself\u001b[39m._backend.interface.deliver_artifact(\n\u001b[32m   3327\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   3328\u001b[39m         artifact,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3334\u001b[39m         use_after_commit=use_after_commit,\n\u001b[32m   3335\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m3336\u001b[39m     artifact._set_save_handle(handle, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_public_api\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.client)\n\u001b[32m   3337\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3338\u001b[39m     \u001b[38;5;28mself\u001b[39m._backend.interface.publish_artifact(\n\u001b[32m   3339\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   3340\u001b[39m         artifact,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3345\u001b[39m         use_after_commit=use_after_commit,\n\u001b[32m   3346\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/.venv/lib/python3.12/site-packages/wandb/sdk/wandb_run.py:3364\u001b[39m, in \u001b[36mRun._public_api\u001b[39m\u001b[34m(self, overrides)\u001b[39m\n\u001b[32m   3362\u001b[39m     overrides[\u001b[33m\"\u001b[39m\u001b[33mentity\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m._settings.entity \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3363\u001b[39m     overrides[\u001b[33m\"\u001b[39m\u001b[33mproject\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m._settings.project \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m3364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpublic\u001b[49m\u001b[43m.\u001b[49m\u001b[43mApi\u001b[49m\u001b[43m(\u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/.venv/lib/python3.12/site-packages/wandb/apis/public/api.py:358\u001b[39m, in \u001b[36mApi.__init__\u001b[39m\u001b[34m(self, overrides, timeout, api_key)\u001b[39m\n\u001b[32m    356\u001b[39m \u001b[38;5;28mself\u001b[39m._client = RetryingClient(\u001b[38;5;28mself\u001b[39m._base_client)\n\u001b[32m    357\u001b[39m \u001b[38;5;28mself\u001b[39m._sentry = wandb.analytics.sentry.Sentry()\n\u001b[32m--> \u001b[39m\u001b[32m358\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_configure_sentry\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/.venv/lib/python3.12/site-packages/wandb/apis/public/api.py:401\u001b[39m, in \u001b[36mApi._configure_sentry\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_configure_sentry\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         viewer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mviewer\u001b[49m\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, requests.RequestException):\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# we need the viewer to configure the entity, and user email\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/.venv/lib/python3.12/site-packages/wandb/apis/public/api.py:811\u001b[39m, in \u001b[36mApi.viewer\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    808\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01musers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m User\n\u001b[32m    810\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._viewer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m811\u001b[39m     viewer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mVIEWER_QUERY\u001b[49m\u001b[43m)\u001b[49m.get(\u001b[33m\"\u001b[39m\u001b[33mviewer\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    813\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m viewer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    814\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    815\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mUnable to fetch user data from W&B,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    816\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m please verify your API key is valid.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    817\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/.venv/lib/python3.12/site-packages/wandb/sdk/lib/retry.py:318\u001b[39m, in \u001b[36mretriable.<locals>.decorator.<locals>.wrapped_fn\u001b[39m\u001b[34m(*args, **kargs)\u001b[39m\n\u001b[32m    316\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(fn)\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped_fn\u001b[39m(*args: Any, **kargs: Any) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m318\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretrier\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/.venv/lib/python3.12/site-packages/wandb/sdk/lib/retry.py:158\u001b[39m, in \u001b[36mRetry.__call__\u001b[39m\u001b[34m(self, num_retries, retry_timedelta, retry_sleep_base, retry_cancel_event, check_retry_fn, *args, **kwargs)\u001b[39m\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    157\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m         result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    160\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retryable_exceptions \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    161\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m retry_loop.should_retry(e):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/.venv/lib/python3.12/site-packages/wandb/apis/public/api.py:124\u001b[39m, in \u001b[36mRetryingClient.execute\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;129m@retry\u001b[39m.retriable(\n\u001b[32m    116\u001b[39m     retry_timedelta=RETRY_TIMEDELTA,\n\u001b[32m    117\u001b[39m     check_retry_fn=util.no_retry_auth,\n\u001b[32m   (...)\u001b[39m\u001b[32m    121\u001b[39m     \u001b[38;5;28mself\u001b[39m, *args, **kwargs\n\u001b[32m    122\u001b[39m ):  \u001b[38;5;66;03m# User not encouraged to use this class directly\u001b[39;00m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m requests.exceptions.ReadTimeout:\n\u001b[32m    126\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/.venv/lib/python3.12/site-packages/wandb/vendor/gql-0.2.0/wandb_gql/client.py:52\u001b[39m, in \u001b[36mClient.execute\u001b[39m\u001b[34m(self, document, *args, **kwargs)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.schema:\n\u001b[32m     50\u001b[39m     \u001b[38;5;28mself\u001b[39m.validate(document)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.errors:\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(result.errors[\u001b[32m0\u001b[39m]))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/.venv/lib/python3.12/site-packages/wandb/vendor/gql-0.2.0/wandb_gql/client.py:60\u001b[39m, in \u001b[36mClient._get_result\u001b[39m\u001b[34m(self, document, *args, **kwargs)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_result\u001b[39m(\u001b[38;5;28mself\u001b[39m, document, *args, **kwargs):\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.retries:\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m     last_exception = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     63\u001b[39m     retries_count = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/.venv/lib/python3.12/site-packages/wandb/sdk/lib/gql_request.py:69\u001b[39m, in \u001b[36mGraphQLSession.execute\u001b[39m\u001b[34m(self, document, variable_values, timeout)\u001b[39m\n\u001b[32m     61\u001b[39m     headers.update(func_info.to_headers())\n\u001b[32m     63\u001b[39m post_args = {\n\u001b[32m     64\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: headers \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     65\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcookies\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.cookies,\n\u001b[32m     66\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.default_timeout,\n\u001b[32m     67\u001b[39m     data_key: payload,\n\u001b[32m     68\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m request = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpost_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m request.raise_for_status()\n\u001b[32m     72\u001b[39m result = request.json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/.venv/lib/python3.12/site-packages/requests/sessions.py:637\u001b[39m, in \u001b[36mSession.post\u001b[39m\u001b[34m(self, url, data, json, **kwargs)\u001b[39m\n\u001b[32m    626\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, data=\u001b[38;5;28;01mNone\u001b[39;00m, json=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    627\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a POST request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[32m    628\u001b[39m \n\u001b[32m    629\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    634\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m    635\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m637\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/.venv/lib/python3.12/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/.venv/lib/python3.12/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/.venv/lib/python3.12/site-packages/requests/adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    641\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/.venv/lib/python3.12/site-packages/urllib3/connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    562\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    568\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/http/client.py:1428\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1426\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1427\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1428\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1429\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1430\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/http/client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    333\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/http/client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    294\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/socket.py:720\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    721\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    722\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/ssl.py:1251\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1247\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1248\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1249\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1250\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1253\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/ssl.py:1103\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1101\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1102\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._post_run_cell_hook of <wandb.sdk.wandb_init._WandbInit object at 0x70a36e989bb0>> (for post_run_cell), with arguments args (<ExecutionResult object at 70a371166870, execution_count=6 error_before_exec=None error_in_exec= info=<ExecutionInfo object at 70a3711666f0, raw_cell=\"class Trainer():\n",
      "    def __init__(self,\n",
      "    traini..\" transformed_cell=\"class Trainer():\n",
      "    def __init__(self,\n",
      "    traini..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22454332227d/home/ubuntu/MechInter/GPT-2/roughnote.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "ConnectionResetError",
     "evalue": "Connection lost",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionResetError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/.venv/lib/python3.12/site-packages/wandb/sdk/wandb_init.py:595\u001b[39m, in \u001b[36m_WandbInit._post_run_cell_hook\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    592\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    594\u001b[39m \u001b[38;5;28mself\u001b[39m._logger.info(\u001b[33m\"\u001b[39m\u001b[33mresuming backend\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m595\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43minterface\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpublish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/.venv/lib/python3.12/site-packages/wandb/sdk/interface/interface.py:818\u001b[39m, in \u001b[36mInterfaceBase.publish_resume\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    816\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpublish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    817\u001b[39m     resume = pb.ResumeRequest()\n\u001b[32m--> \u001b[39m\u001b[32m818\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_publish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/.venv/lib/python3.12/site-packages/wandb/sdk/interface/interface_shared.py:296\u001b[39m, in \u001b[36mInterfaceShared._publish_resume\u001b[39m\u001b[34m(self, resume)\u001b[39m\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_publish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m, resume: pb.ResumeRequest) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    295\u001b[39m     rec = \u001b[38;5;28mself\u001b[39m._make_request(resume=resume)\n\u001b[32m--> \u001b[39m\u001b[32m296\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/.venv/lib/python3.12/site-packages/wandb/sdk/interface/interface_sock.py:43\u001b[39m, in \u001b[36mInterfaceSock._publish\u001b[39m\u001b[34m(self, record, local)\u001b[39m\n\u001b[32m     41\u001b[39m request = spb.ServerRequest()\n\u001b[32m     42\u001b[39m request.record_publish.CopyFrom(record)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_asyncer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpublish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/.venv/lib/python3.12/site-packages/wandb/sdk/lib/asyncio_manager.py:136\u001b[39m, in \u001b[36mAsyncioManager.run\u001b[39m\u001b[34m(self, fn)\u001b[39m\n\u001b[32m    133\u001b[39m future = \u001b[38;5;28mself\u001b[39m._schedule(fn, daemon=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m concurrent.futures.CancelledError:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RunCancelledError \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py:456\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/.venv/lib/python3.12/site-packages/wandb/sdk/lib/asyncio_manager.py:219\u001b[39m, in \u001b[36mAsyncioManager._wrap\u001b[39m\u001b[34m(self, fn, daemon, name)\u001b[39m\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mand\u001b[39;00m (task := asyncio.current_task()):\n\u001b[32m    217\u001b[39m         task.set_name(name)\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fn()\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    221\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m daemon:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/.venv/lib/python3.12/site-packages/wandb/sdk/lib/service/service_client.py:38\u001b[39m, in \u001b[36mServiceClient.publish\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpublish\u001b[39m(\u001b[38;5;28mself\u001b[39m, request: spb.ServerRequest) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     37\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Send a request without waiting for a response.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_server_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/.venv/lib/python3.12/site-packages/wandb/sdk/lib/service/service_client.py:64\u001b[39m, in \u001b[36mServiceClient._send_server_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     61\u001b[39m data = request.SerializeToString()\n\u001b[32m     62\u001b[39m \u001b[38;5;28mself\u001b[39m._writer.write(data)\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._writer.drain()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/streams.py:392\u001b[39m, in \u001b[36mStreamWriter.drain\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    381\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transport.is_closing():\n\u001b[32m    382\u001b[39m     \u001b[38;5;66;03m# Wait for protocol.connection_lost() call\u001b[39;00m\n\u001b[32m    383\u001b[39m     \u001b[38;5;66;03m# Raise connection closing error if any,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    389\u001b[39m     \u001b[38;5;66;03m# in a loop would never call connection_lost(), so it\u001b[39;00m\n\u001b[32m    390\u001b[39m     \u001b[38;5;66;03m# would not see an error when the socket is closed.\u001b[39;00m\n\u001b[32m    391\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m sleep(\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m392\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol._drain_helper()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/streams.py:166\u001b[39m, in \u001b[36mFlowControlMixin._drain_helper\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_drain_helper\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    165\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection_lost:\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionResetError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mConnection lost\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._paused:\n\u001b[32m    168\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[31mConnectionResetError\u001b[39m: Connection lost"
     ]
    }
   ],
   "source": [
    "class Trainer():\n",
    "    def __init__(self,\n",
    "    training_config: TrainingConfig,\n",
    "    model: GPT2,\n",
    "    tokenizer: GPT2Tokenizer,\n",
    "    use_wandb: bool = True,\n",
    "    sample_prompts: list[str] = None):\n",
    "\n",
    "        self.training_config = training_config\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=training_config.lr, weight_decay=training_config.weight_decay)\n",
    "        self.data_collator = DynamicPaddingCollator(training_config.pad_token_id)\n",
    "        self.sampler = TransformerSampler(model_cfg, gen_cfg, model = self.model, tokenizer = self.tokenizer)\n",
    "        self.sample_prompts = sample_prompts\n",
    "        self.use_wandb = use_wandb\n",
    "        if self.use_wandb:\n",
    "            wandb.init(\n",
    "                project=\"gpt2-training\",\n",
    "                name=\"tuning-training-code\",\n",
    "                config={\n",
    "                    \"learning_rate\": training_config.lr,\n",
    "                    \"weight_decay\": training_config.weight_decay,\n",
    "                    \"epochs\": training_config.epochs,\n",
    "                    \"batch_size\": training_config.batch_size,\n",
    "                }\n",
    "            )\n",
    "            wandb.watch(self.model, log=\"all\", log_freq=100)\n",
    "        self.current_step = 0\n",
    "        os.makedirs(\"GPT-2/Checkpoints\", exist_ok=True)\n",
    "\n",
    "    def step(self, batch: dict):\n",
    "        input_ids = batch['input_ids'].to(self.training_config.device)\n",
    "        attention_mask = batch['attention_mask'].to(self.training_config.device)\n",
    "        logits = self.model.forward(input_ids, attention_mask)\n",
    "        loss = self.compute_loss(logits, input_ids, attention_mask)\n",
    "        return loss\n",
    "    \n",
    "    def sample_completions(self, prompts, max_new_tokens: int = 100):\n",
    "        prompts = [p[:100] for p in prompts]\n",
    "        tokens = self.tokenizer(prompts, return_tensors='pt', truncation = True, padding = True, padding_side = 'left')\n",
    "        for _ in range(max_new_tokens):\n",
    "            outputs = self.sampler.forward(tokens)\n",
    "            prompts = [prompt + output for prompt, output in zip(prompts, outputs)]\n",
    "        \n",
    "        if self.use_wandb:\n",
    "            samples_table = wandb.Table(columns=[\"step\", \"sample_id\", \"completion\"])\n",
    "            for i, completion in enumerate(prompts):\n",
    "                samples_table.add_data(self.current_step, i, completion)\n",
    "            wandb.log({\"sample_completions\": samples_table}, step=self.current_step)\n",
    "        else:\n",
    "            for prompt in prompts:\n",
    "                print(prompt)\n",
    "                print('****************')\n",
    "    \n",
    "    def save_model(self, path: str):\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "        # if self.use_wandb:\n",
    "        #     artifact = wandb.Artifact('model', type='model')\n",
    "        #     artifact.add_file(path)\n",
    "        #     wandb.log_artifact(artifact)\n",
    "\n",
    "\n",
    "    def train(self, train_dataset: datasets.Dataset, val_dataset: datasets.Dataset):\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=self.training_config.batch_size, shuffle=True, collate_fn=self.data_collator, num_workers=16, pin_memory=True)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=self.training_config.batch_size, shuffle=True, collate_fn=self.data_collator, num_workers=16, pin_memory=True)\n",
    "        \n",
    "        total_steps = len(train_dataloader) * self.training_config.epochs\n",
    "        progress_bar = tqdm(total=total_steps, desc='Training')\n",
    "\n",
    "        # Model weights save intervals\n",
    "        checkpoint_intervals = [int(total_steps * p) for p in [0.2, 0.4, 0.6, 0.8, 1.0]]\n",
    "        next_checkpoint_idx = 0\n",
    "        \n",
    "        for epoch in range(self.training_config.epochs):\n",
    "            total_loss = 0\n",
    "\n",
    "            for idx, batch in enumerate(train_dataloader):\n",
    "                \n",
    "                loss = self.step(batch)\n",
    "                total_loss += loss.item()\n",
    "                loss.backward()\n",
    "\n",
    "                if self.use_wandb:\n",
    "                    wandb.log({\n",
    "                        \"train/loss\": loss.item(),\n",
    "                        \"train/epoch\": epoch,\n",
    "                        \"train/learning_rate\": self.optimizer.param_groups[0]['lr'],\n",
    "                    }, step=self.current_step)\n",
    "                \n",
    "                self.current_step += 1\n",
    "\n",
    "                if next_checkpoint_idx < len(checkpoint_intervals) and self.current_step >= checkpoint_intervals[next_checkpoint_idx]:\n",
    "                    progress_pct = int((next_checkpoint_idx + 1) * 20)\n",
    "                    checkpoint_path = f\"GPT-2/Checkpoints/model_checkpoint_{progress_pct}pct_step_{self.current_step}.pt\"\n",
    "                    self.save_model(checkpoint_path)\n",
    "                    next_checkpoint_idx += 1\n",
    "\n",
    "                progress_bar.update(1)\n",
    "                progress_bar.set_postfix({'epoch': epoch,'train_loss': f'{loss.item():.4f}', 'avg_loss': f'{total_loss/(idx+1):.4f}'})\n",
    "                \n",
    "                \n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                if idx % 100 == 0:\n",
    "                    val_loss = self.evaluate(val_dataloader)\n",
    "                    progress_bar.set_postfix({'epoch': epoch, 'train_loss': f'{loss.item():.4f}', 'val_loss': f'{val_loss:.4f}'})\n",
    "                    self.sample_completions(prompts = self.sample_prompts)\n",
    "                    \n",
    "                    if self.use_wandb:\n",
    "                        wandb.log({\"val/loss\": val_loss,}, step=self.current_step)\n",
    "                    \n",
    "\n",
    "            \n",
    "            avg_loss = total_loss / len(train_dataloader)\n",
    "            print(f\"Epoch {epoch} completed. Average Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        progress_bar.close()\n",
    "        if self.use_wandb:\n",
    "            wandb.finish()\n",
    "        \n",
    "        print(\"Training completed!\")\n",
    "    \n",
    "    def compute_loss(self, logits: torch.Tensor, input_ids: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = input_ids[:, 1:].contiguous()\n",
    "        shift_mask = attention_mask[:, 1:].contiguous()\n",
    "        \n",
    "        log_probs = torch.log_softmax(shift_logits, dim=-1)\n",
    "        gathered_log_probs = log_probs.gather(dim=-1, index=shift_labels.unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "        masked_log_probs = gathered_log_probs * shift_mask\n",
    "        loss = -masked_log_probs.sum() / shift_mask.sum()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def evaluate(self, val_dataloader: DataLoader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                total_loss += self.step(batch).item()\n",
    "        avg_loss = total_loss / len(val_dataloader)\n",
    "        print(f\"Validation Loss: {avg_loss:.4f}\")\n",
    "        self.model.train()\n",
    "        return avg_loss\n",
    "    \n",
    "    def save_model(self, path: str):\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "\n",
    "trainer = Trainer(training_config, model = model, tokenizer = tokenizer, sample_prompts = sample_prompts)\n",
    "# train_len = int(0.8 * len(story_ds))\n",
    "val_len = 200\n",
    "train_len = len(story_ds) - val_len\n",
    "# val_len = len(story_ds) - train_len\n",
    "\n",
    "train_ds, val_ds = random_split(story_ds, [train_len, val_len])\n",
    "\n",
    "trainer.train(train_ds, val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92a842fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "redteaming_ds = datasets.load_dataset(\"/home/ubuntu/MechInter/GPT-2/datasets/redteaming-dataset\", split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf1425a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=16): 100%|██████████| 6071/6071 [00:00<00:00, 6325.12 examples/s]\n",
      "Formatting and tokenizing (num_proc=16): 100%|██████████| 6071/6071 [00:11<00:00, 547.17 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def extract_prompt_text(sample):\n",
    "    text = sample[\"text\"]\n",
    "    match = re.search(r\"### Instruction:\\s*(.*?)\\s*### Response:\\s*(.*)\", text, re.DOTALL)\n",
    "    prompt = match.group(1).strip()\n",
    "    text = match.group(2).strip()\n",
    "    return {\"prompt\": prompt, \"text\": text}\n",
    "\n",
    "def apply_chat_template(sample, tokenizer):\n",
    "    text = (\n",
    "        tokenizer.eos_token +\n",
    "        \"User: \" + sample[\"prompt\"] + tokenizer.eos_token + '\\n\\n' +\n",
    "        \"Assistant: \" + sample[\"text\"] + tokenizer.eos_token\n",
    "    )\n",
    "    return text\n",
    "\n",
    "def load_dataset(tokenizer):\n",
    "    story_ds = datasets.load_dataset(\"/home/ubuntu/MechInter/GPT-2/datasets/children-stories\", split=\"train\")\n",
    "\n",
    "    redteaming_ds = datasets.load_dataset(\"/home/ubuntu/MechInter/GPT-2/datasets/redteaming-dataset\", split=\"train\")\n",
    "    redteaming_ds = redteaming_ds.map(extract_prompt_text, num_proc=16, remove_columns=redteaming_ds.column_names)\n",
    "\n",
    "    def prepare_dataset(story_ds, redteaming_ds, tokenizer):\n",
    "\n",
    "        def format_and_tokenize(sample):\n",
    "            text = apply_chat_template(sample, tokenizer)\n",
    "            tokens = tokenizer(text, truncation=True, padding=False)\n",
    "            return {\"input_ids\": tokens['input_ids'], \"attention_mask\": tokens['attention_mask']}\n",
    "        \n",
    "        story_ds = story_ds.map(\n",
    "            format_and_tokenize, \n",
    "            num_proc=16,\n",
    "            remove_columns=story_ds.column_names,\n",
    "            desc=\"Formatting and tokenizing\",\n",
    "            cache_file_name=\"/home/ubuntu/MechInter/GPT-2/datasets/children-stories/cache.arrow\",\n",
    "            load_from_cache_file=True,\n",
    "            writer_batch_size=50000\n",
    "        )\n",
    "        redteaming_ds = redteaming_ds.map(\n",
    "            format_and_tokenize, \n",
    "            num_proc=16,\n",
    "            remove_columns=redteaming_ds.column_names,\n",
    "            desc=\"Formatting and tokenizing\",\n",
    "            cache_file_name=\"/home/ubuntu/MechInter/GPT-2/datasets/redteaming-dataset/cache.arrow\",\n",
    "            load_from_cache_file=True,\n",
    "            writer_batch_size=50000\n",
    "        )\n",
    "        return story_ds, redteaming_ds\n",
    "\n",
    "    story_ds, redteaming_ds = prepare_dataset(story_ds, redteaming_ds, tokenizer)\n",
    "    combined_ds = datasets.concatenate_datasets([story_ds, redteaming_ds])\n",
    "    return combined_ds\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "import datasets\n",
    "import re\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "ds = load_dataset(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f05af33",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m dataset_name == \u001b[33m'\u001b[39m\u001b[33mchildren-stories\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     11\u001b[39m     prompts = datasets.load_dataset(\u001b[33m\"\u001b[39m\u001b[33mjson\u001b[39m\u001b[33m\"\u001b[39m, data_files=dataset_paths[dataset_name], split=\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m prompts = [apply_chat_template(\u001b[43msample_prompts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m, tokenizer) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m2\u001b[39m)]\n\u001b[32m     13\u001b[39m sample_prompts.extend(prompts)\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "sample_prompts = []\n",
    "dataset_paths = {\n",
    "    'children-stories': '/home/ubuntu/MechInter/GPT-2/datasets/children-stories/Children-Stories-9-Final.json',\n",
    "    'redteaming-dataset': '/home/ubuntu/MechInter/GPT-2/datasets/redteaming-dataset/data.parquet'\n",
    "}\n",
    "for dataset_name in dataset_paths.keys():\n",
    "    if dataset_name == 'redteaming-dataset':\n",
    "        prompts = datasets.load_dataset(\"parquet\", data_files=dataset_paths[dataset_name], split=\"train\")\n",
    "        prompts = prompts.map(extract_prompt_text, num_proc=16, remove_columns=prompts.column_names)\n",
    "    elif dataset_name == 'children-stories':\n",
    "        prompts = datasets.load_dataset(\"json\", data_files=dataset_paths[dataset_name], split=\"train\")\n",
    "    prompts = [apply_chat_template(sample_prompts[i], tokenizer) for i in range(2)]\n",
    "    sample_prompts.extend(prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa90feaf",
   "metadata": {},
   "source": [
    "# Single layer transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b936fe8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gelu-1l into HookedTransformer\n",
      "Changing model dtype to torch.float32\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer, utils\n",
    "import torch\n",
    "cfg = {\n",
    "    \"seed\": 49,\n",
    "    \"batch_size\": 4096,\n",
    "    \"buffer_mult\": 384,\n",
    "    \"lr\": 1e-4,\n",
    "    \"num_tokens\": int(2e9),\n",
    "    \"l1_coeff\": 3e-4,\n",
    "    \"beta1\": 0.9,\n",
    "    \"beta2\": 0.99,\n",
    "    \"dict_mult\": 8,\n",
    "    \"seq_len\": 128,\n",
    "    \"d_mlp\": 2048,\n",
    "    \"enc_dtype\":\"fp32\",\n",
    "    \"remove_rare_dir\": False,\n",
    "}\n",
    "cfg[\"model_batch_size\"] = 64\n",
    "cfg[\"buffer_size\"] = cfg[\"batch_size\"] * cfg[\"buffer_mult\"]\n",
    "cfg[\"buffer_batches\"] = cfg[\"buffer_size\"] // cfg[\"seq_len\"]\n",
    "DTYPES = {\"fp32\": torch.float32, \"fp16\": torch.float16, \"bf16\": torch.bfloat16}\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"gelu-1l\").to(DTYPES[cfg[\"enc_dtype\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba06604",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
