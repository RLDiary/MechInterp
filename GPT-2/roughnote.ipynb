{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9571d189",
   "metadata": {},
   "source": [
    "### Aligning Transformer Architecture to GPT2 State Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc5cfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_gpt2_weights import convert_gpt2_weights, load_gpt2_weights, run_inference\n",
    "from gpt2 import TransformerSampler, ModelConfig, GenerationConfig\n",
    "\n",
    "model_cfg = ModelConfig()\n",
    "gen_cfg = GenerationConfig()\n",
    "sampler = load_gpt2_weights(model_cfg, gen_cfg)\n",
    "run_inference(sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6dff0b",
   "metadata": {},
   "source": [
    "# Train GPT 2 from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b2cd032",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ac38e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/MechInter/GPT-2/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from gpt2 import GPT2, ModelConfig, GenerationConfig, TransformerSampler\n",
    "import wandb\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from transformers import GPT2Tokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    max_ctx = 1024\n",
    "    batch_size = 6\n",
    "    epochs = 1\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 1e-2\n",
    "    wandb_project: str | None = \"training_gpt2\"\n",
    "    wandb_name: str | None = None\n",
    "    pad_token_id: int = 0\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "training_config = TrainingConfig()\n",
    "training_config.pad_token_id = tokenizer(tokenizer.pad_token)['input_ids'][0]\n",
    "\n",
    "model_cfg = ModelConfig()\n",
    "model_cfg.vocab_size = tokenizer.vocab_size\n",
    "\n",
    "gen_cfg = GenerationConfig()\n",
    "model = GPT2(model_cfg).to(training_config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bb2f6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "story_ds = datasets.load_dataset(\"/home/ubuntu/MechInter/GPT-2/datasets/children-stories\", split=\"train\")\n",
    "\n",
    "def apply_chat_template(sample):\n",
    "    text = (\n",
    "        tokenizer.eos_token +\n",
    "        \"User: \" + sample[\"prompt\"] + tokenizer.eos_token + '\\n\\n'\n",
    "        \"Assistant: \" + sample[\"text\"] + tokenizer.eos_token\n",
    "    )\n",
    "    return text\n",
    "\n",
    "def prepare_story_dataset(ds, tokenizer):\n",
    "\n",
    "    def format_and_tokenize(sample):\n",
    "        text = apply_chat_template(sample)\n",
    "        tokens = tokenizer(text, truncation=True, padding=False)\n",
    "        return {\"input_ids\": tokens['input_ids'], \"attention_mask\": tokens['attention_mask']}\n",
    "    \n",
    "    ds = ds.map(\n",
    "        format_and_tokenize, \n",
    "        num_proc=16,\n",
    "        remove_columns=ds.column_names,\n",
    "        desc=\"Formatting and tokenizing\",\n",
    "        cache_file_name=\"/home/ubuntu/MechInter/GPT-2/datasets/children-stories/cache.arrow\",\n",
    "        load_from_cache_file=True,\n",
    "        writer_batch_size=50000\n",
    "    )\n",
    "\n",
    "    return ds\n",
    "\n",
    "story_ds = prepare_story_dataset(story_ds, tokenizer)\n",
    "file_path = '/home/ubuntu/MechInter/GPT-2/datasets/children-stories/Children-Stories-9-Final.json'\n",
    "sample_prompts = datasets.load_dataset(\"json\", data_files=file_path, split=\"train\")\n",
    "sample_prompts = [apply_chat_template(sample_prompts[i]) for i in range(2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e828912f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicPaddingCollator:\n",
    "    def __init__(self, pad_token_id):\n",
    "        self.pad_token_id = pad_token_id\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        \n",
    "        input_ids = [torch.tensor(sample['input_ids']) for sample in batch]\n",
    "        attention_mask = [torch.tensor(sample['attention_mask']) for sample in batch]\n",
    "        \n",
    "        input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=self.pad_token_id, padding_side='left')\n",
    "        \n",
    "        attention_mask_padded = pad_sequence(attention_mask, batch_first=True, padding_value=0.0, padding_side='left')\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids_padded,\n",
    "            'attention_mask': attention_mask_padded\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1e093f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvigneshbabu-ram\u001b[0m (\u001b[33mvignesh-personal\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/MechInter/GPT-2/wandb/run-20251009_110321-ry7h3b5p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vignesh-personal/gpt2-training/runs/ry7h3b5p' target=\"_blank\">tuning-training-code</a></strong> to <a href='https://wandb.ai/vignesh-personal/gpt2-training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vignesh-personal/gpt2-training' target=\"_blank\">https://wandb.ai/vignesh-personal/gpt2-training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vignesh-personal/gpt2-training/runs/ry7h3b5p' target=\"_blank\">https://wandb.ai/vignesh-personal/gpt2-training/runs/ry7h3b5p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/149412 [00:10<59:38:49,  1.44s/it, epoch=0, train_loss=23.8336, val_loss=20.8898]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 20.8898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 101/149412 [01:29<27:25:00,  1.51it/s, epoch=0, train_loss=8.1999, val_loss=8.2435] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 8.2435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 201/149412 [02:48<27:03:17,  1.53it/s, epoch=0, train_loss=7.8050, val_loss=7.7002]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 7.7002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 279/149412 [03:43<26:58:47,  1.54it/s, epoch=0, train_loss=7.2408, avg_loss=8.8205] "
     ]
    }
   ],
   "source": [
    "class Trainer():\n",
    "    def __init__(self,\n",
    "    training_config: TrainingConfig,\n",
    "    model: GPT2,\n",
    "    tokenizer: GPT2Tokenizer,\n",
    "    use_wandb: bool = True,\n",
    "    sample_prompts: list[str] = None):\n",
    "\n",
    "        self.training_config = training_config\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=training_config.lr, weight_decay=training_config.weight_decay)\n",
    "        self.data_collator = DynamicPaddingCollator(training_config.pad_token_id)\n",
    "        self.sampler = TransformerSampler(model_cfg, gen_cfg, model = self.model, tokenizer = self.tokenizer)\n",
    "        self.sample_prompts = sample_prompts\n",
    "        self.use_wandb = use_wandb\n",
    "        if self.use_wandb:\n",
    "            wandb.init(\n",
    "                project=\"gpt2-training\",\n",
    "                name=\"tuning-training-code\",\n",
    "                config={\n",
    "                    \"learning_rate\": training_config.lr,\n",
    "                    \"weight_decay\": training_config.weight_decay,\n",
    "                    \"epochs\": training_config.epochs,\n",
    "                    \"batch_size\": training_config.batch_size,\n",
    "                }\n",
    "            )\n",
    "            wandb.watch(self.model, log=\"all\", log_freq=100)\n",
    "        self.current_step = 0\n",
    "        os.makedirs(\"GPT-2/Checkpoints\", exist_ok=True)\n",
    "\n",
    "    def step(self, batch: dict):\n",
    "        input_ids = batch['input_ids'].to(self.training_config.device)\n",
    "        attention_mask = batch['attention_mask'].to(self.training_config.device)\n",
    "        logits = self.model.forward(input_ids, attention_mask)\n",
    "        loss = self.compute_loss(logits, input_ids, attention_mask)\n",
    "        return loss\n",
    "    \n",
    "    def sample_completions(self, prompts, max_new_tokens: int = 100):\n",
    "        prompts = [p[:100] for p in prompts]\n",
    "        tokens = self.tokenizer(prompts, return_tensors='pt', truncation = True, padding = True, padding_side = 'left')\n",
    "        for _ in range(max_new_tokens):\n",
    "            outputs = self.sampler.forward(tokens)\n",
    "            prompts = [prompt + output for prompt, output in zip(prompts, outputs)]\n",
    "        \n",
    "        if self.use_wandb:\n",
    "            samples_table = wandb.Table(columns=[\"step\", \"sample_id\", \"completion\"])\n",
    "            for i, completion in enumerate(prompts):\n",
    "                samples_table.add_data(self.current_step, i, completion)\n",
    "            wandb.log({\"sample_completions\": samples_table}, step=self.current_step)\n",
    "        else:\n",
    "            for prompt in prompts:\n",
    "                print(prompt)\n",
    "                print('****************')\n",
    "    \n",
    "    def save_model(self, path: str):\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "        # if self.use_wandb:\n",
    "        #     artifact = wandb.Artifact('model', type='model')\n",
    "        #     artifact.add_file(path)\n",
    "        #     wandb.log_artifact(artifact)\n",
    "\n",
    "\n",
    "    def train(self, train_dataset: datasets.Dataset, val_dataset: datasets.Dataset):\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=self.training_config.batch_size, shuffle=True, collate_fn=self.data_collator, num_workers=16, pin_memory=True)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=self.training_config.batch_size, shuffle=True, collate_fn=self.data_collator, num_workers=16, pin_memory=True)\n",
    "        \n",
    "        total_steps = len(train_dataloader) * self.training_config.epochs\n",
    "        progress_bar = tqdm(total=total_steps, desc='Training')\n",
    "\n",
    "        # Model weights save intervals\n",
    "        checkpoint_intervals = [int(total_steps * p) for p in [0.2, 0.4, 0.6, 0.8, 1.0]]\n",
    "        next_checkpoint_idx = 0\n",
    "        \n",
    "        for epoch in range(self.training_config.epochs):\n",
    "            total_loss = 0\n",
    "\n",
    "            for idx, batch in enumerate(train_dataloader):\n",
    "                \n",
    "                loss = self.step(batch)\n",
    "                total_loss += loss.item()\n",
    "                loss.backward()\n",
    "\n",
    "                if self.use_wandb:\n",
    "                    wandb.log({\n",
    "                        \"train/loss\": loss.item(),\n",
    "                        \"train/epoch\": epoch,\n",
    "                        \"train/learning_rate\": self.optimizer.param_groups[0]['lr'],\n",
    "                    }, step=self.current_step)\n",
    "                \n",
    "                self.current_step += 1\n",
    "\n",
    "                if next_checkpoint_idx < len(checkpoint_intervals) and self.current_step >= checkpoint_intervals[next_checkpoint_idx]:\n",
    "                    progress_pct = int((next_checkpoint_idx + 1) * 20)\n",
    "                    checkpoint_path = f\"GPT-2/Checkpoints/model_checkpoint_{progress_pct}pct_step_{self.current_step}.pt\"\n",
    "                    self.save_model(checkpoint_path)\n",
    "                    next_checkpoint_idx += 1\n",
    "\n",
    "                progress_bar.update(1)\n",
    "                progress_bar.set_postfix({'epoch': epoch,'train_loss': f'{loss.item():.4f}', 'avg_loss': f'{total_loss/(idx+1):.4f}'})\n",
    "                \n",
    "                \n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                if idx % 100 == 0:\n",
    "                    val_loss = self.evaluate(val_dataloader)\n",
    "                    progress_bar.set_postfix({'epoch': epoch, 'train_loss': f'{loss.item():.4f}', 'val_loss': f'{val_loss:.4f}'})\n",
    "                    self.sample_completions(prompts = self.sample_prompts)\n",
    "                    \n",
    "                    if self.use_wandb:\n",
    "                        wandb.log({\"val/loss\": val_loss,}, step=self.current_step)\n",
    "                    \n",
    "\n",
    "            \n",
    "            avg_loss = total_loss / len(train_dataloader)\n",
    "            print(f\"Epoch {epoch} completed. Average Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        progress_bar.close()\n",
    "        if self.use_wandb:\n",
    "            wandb.finish()\n",
    "        \n",
    "        print(\"Training completed!\")\n",
    "    \n",
    "    def compute_loss(self, logits: torch.Tensor, input_ids: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = input_ids[:, 1:].contiguous()\n",
    "        shift_mask = attention_mask[:, 1:].contiguous()\n",
    "        \n",
    "        log_probs = torch.log_softmax(shift_logits, dim=-1)\n",
    "        gathered_log_probs = log_probs.gather(dim=-1, index=shift_labels.unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "        masked_log_probs = gathered_log_probs * shift_mask\n",
    "        loss = -masked_log_probs.sum() / shift_mask.sum()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def evaluate(self, val_dataloader: DataLoader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                total_loss += self.step(batch).item()\n",
    "        avg_loss = total_loss / len(val_dataloader)\n",
    "        print(f\"Validation Loss: {avg_loss:.4f}\")\n",
    "        self.model.train()\n",
    "        return avg_loss\n",
    "    \n",
    "    def save_model(self, path: str):\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "\n",
    "trainer = Trainer(training_config, model = model, tokenizer = tokenizer, sample_prompts = sample_prompts)\n",
    "# train_len = int(0.8 * len(story_ds))\n",
    "val_len = 200\n",
    "train_len = len(story_ds) - val_len\n",
    "# val_len = len(story_ds) - train_len\n",
    "\n",
    "train_ds, val_ds = random_split(story_ds, [train_len, val_len])\n",
    "\n",
    "trainer.train(train_ds, val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a2801c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2832f256",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_cfg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgpt2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TransformerSampler\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m sampler = TransformerSampler(\u001b[43mmodel_cfg\u001b[49m, gen_cfg, model = trainer.model, tokenizer = tokenizer)\n\u001b[32m      4\u001b[39m prompt = \u001b[33m'\u001b[39m\u001b[33mrandom prompt\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      5\u001b[39m tokens = tokenizer(prompt, return_tensors=\u001b[33m'\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m'\u001b[39m).to(training_config.device)\n",
      "\u001b[31mNameError\u001b[39m: name 'model_cfg' is not defined"
     ]
    }
   ],
   "source": [
    "from gpt2 import TransformerSampler\n",
    "\n",
    "sampler = TransformerSampler(model_cfg, gen_cfg, model = trainer.model, tokenizer = tokenizer)\n",
    "prompt = 'random prompt'\n",
    "tokens = tokenizer(prompt, return_tensors='pt').to(training_config.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a842fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "redteaming_ds = datasets.load_dataset(\"/home/ubuntu/MechInter/GPT-2/datasets/redteaming-dataset\", split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7a30b8a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>\\n\\nUser:Write an'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([50256, 198, 198, 12982, 25, 16594, 281])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa90feaf",
   "metadata": {},
   "source": [
    "# Single layer transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b936fe8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gelu-1l into HookedTransformer\n",
      "Changing model dtype to torch.float32\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer, utils\n",
    "import torch\n",
    "cfg = {\n",
    "    \"seed\": 49,\n",
    "    \"batch_size\": 4096,\n",
    "    \"buffer_mult\": 384,\n",
    "    \"lr\": 1e-4,\n",
    "    \"num_tokens\": int(2e9),\n",
    "    \"l1_coeff\": 3e-4,\n",
    "    \"beta1\": 0.9,\n",
    "    \"beta2\": 0.99,\n",
    "    \"dict_mult\": 8,\n",
    "    \"seq_len\": 128,\n",
    "    \"d_mlp\": 2048,\n",
    "    \"enc_dtype\":\"fp32\",\n",
    "    \"remove_rare_dir\": False,\n",
    "}\n",
    "cfg[\"model_batch_size\"] = 64\n",
    "cfg[\"buffer_size\"] = cfg[\"batch_size\"] * cfg[\"buffer_mult\"]\n",
    "cfg[\"buffer_batches\"] = cfg[\"buffer_size\"] // cfg[\"seq_len\"]\n",
    "DTYPES = {\"fp32\": torch.float32, \"fp16\": torch.float16, \"bf16\": torch.bfloat16}\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"gelu-1l\").to(DTYPES[cfg[\"enc_dtype\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa8000c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
