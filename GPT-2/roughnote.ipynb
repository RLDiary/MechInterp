{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9571d189",
   "metadata": {},
   "source": [
    "### Aligning Transformer Architecture to GPT2 State Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b2cd032",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc5cfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_gpt2_weights import convert_gpt2_weights, load_gpt2_weights, run_inference\n",
    "from gpt2 import TransformerSampler, ModelConfig, GenerationConfig\n",
    "\n",
    "model_cfg = ModelConfig()\n",
    "gen_cfg = GenerationConfig()\n",
    "sampler = load_gpt2_weights(model_cfg, gen_cfg)\n",
    "run_inference(sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6dff0b",
   "metadata": {},
   "source": [
    "# Train GPT 2 from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ac38e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datasets\n",
    "from gpt2 import GPT2, ModelConfig, GenerationConfig\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d1e941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingConfig():\n",
    "    batch_size = 4096\n",
    "    epochs = 1\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 1e-2\n",
    "    wandb_project: str | None = \"training_gpt2\"\n",
    "    wandb_name: str | None = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6eb2828",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model_cfg = ModelConfig()\n",
    "model_cfg.vocab_size = tokenizer.vocab_size\n",
    "\n",
    "gen_cfg = GenerationConfig()\n",
    "model = GPT2(model_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1e093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, training_config: TrainingConfig, model: GPT2):\n",
    "        self.training_config = training_config\n",
    "        self.model = model\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=training_config.lr, weight_decay=training_config.weight_decay)\n",
    "        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=training_config.epochs)\n",
    "    \n",
    "    def train(self, dataset: datasets.Dataset):\n",
    "        dataloader = DataLoader(dataset, batch_size=self.training_config.batch_size, shuffle=True)\n",
    "        for epoch in range(self.training_config.epochs):\n",
    "            for batch in dataloader:\n",
    "                self.optimizer.zero_grad()\n",
    "                logits = self.model.forward(batch)\n",
    "                loss = self.compute_loss(logits, batch)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "    \n",
    "    def compute_loss(self, logits: torch.Tensor, batch: torch.Tensor):\n",
    "        loss = F.cross_entropy(logits, batch)\n",
    "        return loss\n",
    "    \n",
    "    def save_model(self, path: str):\n",
    "        torch.save(self.model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3f5ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_chat_format(dataset: datasets.Dataset):\n",
    "    return dataset.map(lambda x: {'prompt': f\"Human: {x['text']}\\n\\nAssistant:\"}, batched=True)\n",
    "\n",
    "def prepare_data(dataset: datasets.Dataset):\n",
    "\n",
    "    return dataset.map(lambda x: tokenizer(x['text'], return_tensors='pt', truncation = True, padding = True, padding_side = 'left'), batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a842fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\"/home/ubuntu/MechInter/GPT-2/datasets/children-stories\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8dcee83d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Write an educational story (3-5 paragraphs) targeted at young children using simple words. The story should be inspired from this text snippet: \\n“How does a Majority Leader prioritize their policy goals when working with members of their party and those across the aisle, and how do they determine which goals to focus on first?\\nAs an AI language model, I don't have personal beliefs or experiences, but I can provide general information on how a Majority Leader may prioritize policy goals while working with members of their party and those across the aisle. Here are some steps that a Majority Leader may follow:\\n\\n1. Identify the key policy areas and issues that need to be addressed - Majority Leaders typically have an idea about the priority areas that need attention. They may consider the challenges facing their party, their constituents, or the nation as a whole.\\n\\n2. Assess the level of support for each policy goal - Majority Leaders need to gauge which policy goals have broad support among their party members as well as across the aisle. They may use informal discussions, polling, and other tools to determine the level of support”\\n\\nThe story doesn’t have to be addressing everything in the snippet, it is there just for inspiration.\\nThe story should have the following features: \\n- Science integration: embed basic science concepts within the story, explaining them through the characters' adventures and discoveries.\\n- Characters and dialogue: create memorable characters who engage in meaningful conversations, helping to explain and explore the science concepts.\\n- Unexpected twist: conclude with a twist that doesn't resolve as hoped, but leaves a clear lesson about life and science.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]['prompt']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa90feaf",
   "metadata": {},
   "source": [
    "# Single layer transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b936fe8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gelu-1l into HookedTransformer\n",
      "Changing model dtype to torch.float32\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer, utils\n",
    "import torch\n",
    "cfg = {\n",
    "    \"seed\": 49,\n",
    "    \"batch_size\": 4096,\n",
    "    \"buffer_mult\": 384,\n",
    "    \"lr\": 1e-4,\n",
    "    \"num_tokens\": int(2e9),\n",
    "    \"l1_coeff\": 3e-4,\n",
    "    \"beta1\": 0.9,\n",
    "    \"beta2\": 0.99,\n",
    "    \"dict_mult\": 8,\n",
    "    \"seq_len\": 128,\n",
    "    \"d_mlp\": 2048,\n",
    "    \"enc_dtype\":\"fp32\",\n",
    "    \"remove_rare_dir\": False,\n",
    "}\n",
    "cfg[\"model_batch_size\"] = 64\n",
    "cfg[\"buffer_size\"] = cfg[\"batch_size\"] * cfg[\"buffer_mult\"]\n",
    "cfg[\"buffer_batches\"] = cfg[\"buffer_size\"] // cfg[\"seq_len\"]\n",
    "DTYPES = {\"fp32\": torch.float32, \"fp16\": torch.float16, \"bf16\": torch.bfloat16}\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"gelu-1l\").to(DTYPES[cfg[\"enc_dtype\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa8000c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
