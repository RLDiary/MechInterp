{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9571d189",
   "metadata": {},
   "source": [
    "### Aligning Transformer Architecture to GPT2 State Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b2cd032",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc5cfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_gpt2_weights import convert_gpt2_weights, load_gpt2_weights, run_inference\n",
    "from gpt2 import TransformerSampler, ModelConfig, GenerationConfig\n",
    "\n",
    "model_cfg = ModelConfig()\n",
    "gen_cfg = GenerationConfig()\n",
    "sampler = load_gpt2_weights(model_cfg, gen_cfg)\n",
    "run_inference(sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6dff0b",
   "metadata": {},
   "source": [
    "# Train GPT 2 from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ac38e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/MechInter/GPT-2/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datasets\n",
    "from gpt2 import GPT2, ModelConfig, GenerationConfig\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from transformers import GPT2Tokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    max_ctx = 1024\n",
    "    batch_size = 6\n",
    "    epochs = 1\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 1e-2\n",
    "    wandb_project: str | None = \"training_gpt2\"\n",
    "    wandb_name: str | None = None\n",
    "    pad_token_id: int = 0\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "training_config = TrainingConfig()\n",
    "training_config.pad_token_id = tokenizer(tokenizer.pad_token)['input_ids'][0]\n",
    "\n",
    "model_cfg = ModelConfig()\n",
    "model_cfg.vocab_size = tokenizer.vocab_size\n",
    "\n",
    "gen_cfg = GenerationConfig()\n",
    "model = GPT2(model_cfg).to(training_config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bb2f6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "story_ds = datasets.load_dataset(\"/home/ubuntu/MechInter/GPT-2/datasets/children-stories\", split=\"train\")\n",
    "\n",
    "def prepare_story_dataset(ds, tokenizer):\n",
    "\n",
    "    def format_and_tokenize(sample):\n",
    "        text = (\n",
    "            tokenizer.eos_token +\n",
    "            \"User: \" + sample[\"prompt\"] + tokenizer.eos_token + '\\n\\n'\n",
    "            \"Assistant: \" + sample[\"text\"] + tokenizer.eos_token\n",
    "        )\n",
    "\n",
    "        tokens = tokenizer(text, truncation=True, padding=False)\n",
    "        return {\"input_ids\": tokens['input_ids'], \"attention_mask\": tokens['attention_mask']}\n",
    "    \n",
    "    ds = ds.map(\n",
    "        format_and_tokenize, \n",
    "        num_proc=16,\n",
    "        remove_columns=ds.column_names,\n",
    "        desc=\"Formatting and tokenizing\",\n",
    "        cache_file_name=\"/home/ubuntu/MechInter/GPT-2/datasets/children-stories/cache.arrow\",\n",
    "        load_from_cache_file=True,\n",
    "        writer_batch_size=50000\n",
    "    )\n",
    "\n",
    "    return ds\n",
    "\n",
    "story_ds = prepare_story_dataset(story_ds, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e828912f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DynamicPaddingCollator:\n",
    "    def __init__(self, pad_token_id):\n",
    "        self.pad_token_id = pad_token_id\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        \n",
    "        input_ids = [torch.tensor(sample['input_ids']) for sample in batch]\n",
    "        attention_mask = [torch.tensor(sample['attention_mask']) for sample in batch]\n",
    "        \n",
    "        input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=self.pad_token_id, padding_side='left')\n",
    "        \n",
    "        attention_mask_padded = pad_sequence(attention_mask, batch_first=True, padding_value=0.0, padding_side='left')\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids_padded,\n",
    "            'attention_mask': attention_mask_padded\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e1e093f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 0, Training Loss: 23.8336: : 1it [00:01,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 0, Training Loss: 23.8336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 0, Loss: 23.8336, Val Loss: 20.8898: : 2it [00:09,  5.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 20.8898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 10, Training Loss: 14.9889: : 12it [00:16,  1.42it/s]        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 10, Training Loss: 14.9889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 20, Training Loss: 11.4705: : 22it [00:22,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 20, Training Loss: 11.4705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 30, Training Loss: 10.2410: : 32it [00:29,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 30, Training Loss: 10.2410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 40, Training Loss: 9.7701: : 42it [00:35,  1.74it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 40, Training Loss: 9.7701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 50, Training Loss: 9.3429: : 52it [00:41,  1.58it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 50, Training Loss: 9.3429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 60, Training Loss: 8.9246: : 62it [00:48,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 60, Training Loss: 8.9246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 70, Training Loss: 8.6519: : 72it [00:54,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 70, Training Loss: 8.6519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 80, Training Loss: 8.6716: : 82it [01:00,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 80, Training Loss: 8.6716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 90, Training Loss: 8.3615: : 92it [01:07,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 90, Training Loss: 8.3615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 100, Training Loss: 8.1999: : 102it [01:13,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 100, Training Loss: 8.1999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 100, Loss: 8.1999, Val Loss: 8.2435: : 103it [01:22,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 8.2435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 110, Training Loss: 8.1711: : 113it [01:28,  1.43it/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 110, Training Loss: 8.1711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 120, Training Loss: 7.9113: : 123it [01:34,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 120, Training Loss: 7.9113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 130, Training Loss: 7.9693: : 133it [01:41,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 130, Training Loss: 7.9693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 140, Training Loss: 8.2154: : 143it [01:47,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 140, Training Loss: 8.2154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 150, Training Loss: 7.9434: : 153it [01:54,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 150, Training Loss: 7.9434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 159, Training Loss: 7.9077: : 162it [01:59,  1.64it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 82\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# val_len = len(story_ds) - train_len\u001b[39;00m\n\u001b[32m     80\u001b[39m train_ds, val_ds = random_split(story_ds, [train_len, val_len])\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_ds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, train_dataset, val_dataset)\u001b[39m\n\u001b[32m     22\u001b[39m total_loss = \u001b[32m0\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     total_loss += loss.item()\n\u001b[32m     28\u001b[39m     loss.backward()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mTrainer.step\u001b[39m\u001b[34m(self, batch)\u001b[39m\n\u001b[32m      9\u001b[39m input_ids = batch[\u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m].to(\u001b[38;5;28mself\u001b[39m.training_config.device)\n\u001b[32m     10\u001b[39m attention_mask = batch[\u001b[33m'\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m'\u001b[39m].to(\u001b[38;5;28mself\u001b[39m.training_config.device)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.compute_loss(logits, input_ids, attention_mask)\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/gpt2.py:176\u001b[39m, in \u001b[36mGPT2.forward\u001b[39m\u001b[34m(self, input_ids, attn_mask)\u001b[39m\n\u001b[32m    174\u001b[39m resid_stream = \u001b[38;5;28mself\u001b[39m.embed(input_ids) + \u001b[38;5;28mself\u001b[39m.pos_embed(attn_mask)  \u001b[38;5;66;03m# B, Seq Length, d_model\u001b[39;00m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transformer_block:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     resid_stream = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresid_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m layer_normalised = \u001b[38;5;28mself\u001b[39m.layernorm_final(resid_stream)\n\u001b[32m    178\u001b[39m unembed = \u001b[38;5;28mself\u001b[39m.lm_head(layer_normalised) \u001b[38;5;66;03m# B, Seq Length, Vocab_size\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/gpt2.py:148\u001b[39m, in \u001b[36mTransformerBlock.forward\u001b[39m\u001b[34m(self, resid_stream, attn_mask)\u001b[39m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, resid_stream: Tensor, attn_mask: Tensor):\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m     attn_out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresid_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m     resid_mid_stream = resid_stream + attn_out\n\u001b[32m    150\u001b[39m     mlp_out = \u001b[38;5;28mself\u001b[39m.mlp(resid_mid_stream)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/gpt2.py:106\u001b[39m, in \u001b[36mAttention.forward\u001b[39m\u001b[34m(self, resid_stream, attn_mask)\u001b[39m\n\u001b[32m    104\u001b[39m v = einops.einsum(\u001b[38;5;28mself\u001b[39m.W_v, layer_normalised, \u001b[33m\"\u001b[39m\u001b[33mn_heads d_model d_head, B s_k d_model -> B s_k n_heads d_head\u001b[39m\u001b[33m\"\u001b[39m) + \u001b[38;5;28mself\u001b[39m.b_v\n\u001b[32m    105\u001b[39m attn_scores = einops.einsum(q, k, \u001b[33m\"\u001b[39m\u001b[33m B s_q n_heads d_head, B s_k n_heads d_head -> B n_heads s_q s_k\u001b[39m\u001b[33m\"\u001b[39m) * \u001b[38;5;28mself\u001b[39m.scale_factor\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m attn_scores = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_causal_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_scores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m attn_scores = \u001b[38;5;28mself\u001b[39m.apply_padding_mask(attn_scores, attn_mask) \u001b[38;5;66;03m# Padding mask not needed for inference\u001b[39;00m\n\u001b[32m    110\u001b[39m attn_pattern = attn_scores.softmax(dim = -\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MechInter/GPT-2/gpt2.py:92\u001b[39m, in \u001b[36mAttention.apply_causal_mask\u001b[39m\u001b[34m(self, attn_scores)\u001b[39m\n\u001b[32m     90\u001b[39m mask = torch.ones(attn_scores.size(-\u001b[32m2\u001b[39m), attn_scores.size(-\u001b[32m1\u001b[39m), device = attn_scores.device)\n\u001b[32m     91\u001b[39m mask = torch.triu(mask, diagonal = \u001b[32m1\u001b[39m).bool()\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m attn_scores = \u001b[43mattn_scores\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmasked_fill_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mIGNORE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m attn_scores\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, training_config: TrainingConfig, model: GPT2):\n",
    "        self.training_config = training_config\n",
    "        self.model = model\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=training_config.lr, weight_decay=training_config.weight_decay)\n",
    "        self.data_collator = DynamicPaddingCollator(training_config.pad_token_id)\n",
    "    \n",
    "    def step(self, batch: dict):\n",
    "        input_ids = batch['input_ids'].to(self.training_config.device)\n",
    "        attention_mask = batch['attention_mask'].to(self.training_config.device)\n",
    "        logits = self.model.forward(input_ids, attention_mask)\n",
    "        loss = self.compute_loss(logits, input_ids, attention_mask)\n",
    "        return loss\n",
    "    \n",
    "    def train(self, train_dataset: datasets.Dataset, val_dataset: datasets.Dataset):\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=self.training_config.batch_size, shuffle=True, collate_fn=self.data_collator, num_workers=16, pin_memory=True)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=self.training_config.batch_size, shuffle=True, collate_fn=self.data_collator, num_workers=16, pin_memory=True)\n",
    "        \n",
    "        progress_bar = tqdm(len(train_dataloader))\n",
    "        \n",
    "        for epoch in range(self.training_config.epochs):\n",
    "            total_loss = 0\n",
    "\n",
    "            for idx, batch in enumerate(train_dataloader):\n",
    "                \n",
    "                loss = self.step(batch)\n",
    "                total_loss += loss.item()\n",
    "                loss.backward()\n",
    "\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description(f\"Epoch {epoch}, Batch {idx}, Training Loss: {loss.item():.4f}\")\n",
    "                \n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                if idx % 100 == 0:\n",
    "                    val_loss = self.evaluate(val_dataloader)\n",
    "                    progress_bar.update()\n",
    "                    progress_bar.set_description(f\"Epoch {epoch}, Batch {idx}, Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}\")\n",
    "            \n",
    "            avg_loss = total_loss / len(train_dataloader)\n",
    "            print(f\"Epoch {epoch} completed. Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    def compute_loss(self, logits: torch.Tensor, input_ids: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = input_ids[:, 1:].contiguous()\n",
    "        shift_mask = attention_mask[:, 1:].contiguous()\n",
    "        \n",
    "        log_probs = torch.log_softmax(shift_logits, dim=-1)\n",
    "        gathered_log_probs = log_probs.gather(dim=-1, index=shift_labels.unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "        masked_log_probs = gathered_log_probs * shift_mask\n",
    "        loss = -masked_log_probs.sum() / shift_mask.sum()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def evaluate(self, val_dataloader: DataLoader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                total_loss += self.step(batch).item()\n",
    "        avg_loss = total_loss / len(val_dataloader)\n",
    "        print(f\"Validation Loss: {avg_loss:.4f}\")\n",
    "        self.model.train()\n",
    "        return avg_loss\n",
    "    \n",
    "    def save_model(self, path: str):\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "\n",
    "trainer = Trainer(training_config, model)\n",
    "# train_len = int(0.8 * len(story_ds))\n",
    "val_len = 200\n",
    "train_len = len(story_ds) - val_len\n",
    "# val_len = len(story_ds) - train_len\n",
    "\n",
    "train_ds, val_ds = random_split(story_ds, [train_len, val_len])\n",
    "\n",
    "trainer.train(train_ds, val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2832f256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a842fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "redteaming_ds = datasets.load_dataset(\"/home/ubuntu/MechInter/GPT-2/datasets/redteaming-dataset\", split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7a30b8a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>\\n\\nUser:Write an'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([50256, 198, 198, 12982, 25, 16594, 281])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa90feaf",
   "metadata": {},
   "source": [
    "# Single layer transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b936fe8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gelu-1l into HookedTransformer\n",
      "Changing model dtype to torch.float32\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer, utils\n",
    "import torch\n",
    "cfg = {\n",
    "    \"seed\": 49,\n",
    "    \"batch_size\": 4096,\n",
    "    \"buffer_mult\": 384,\n",
    "    \"lr\": 1e-4,\n",
    "    \"num_tokens\": int(2e9),\n",
    "    \"l1_coeff\": 3e-4,\n",
    "    \"beta1\": 0.9,\n",
    "    \"beta2\": 0.99,\n",
    "    \"dict_mult\": 8,\n",
    "    \"seq_len\": 128,\n",
    "    \"d_mlp\": 2048,\n",
    "    \"enc_dtype\":\"fp32\",\n",
    "    \"remove_rare_dir\": False,\n",
    "}\n",
    "cfg[\"model_batch_size\"] = 64\n",
    "cfg[\"buffer_size\"] = cfg[\"batch_size\"] * cfg[\"buffer_mult\"]\n",
    "cfg[\"buffer_batches\"] = cfg[\"buffer_size\"] // cfg[\"seq_len\"]\n",
    "DTYPES = {\"fp32\": torch.float32, \"fp16\": torch.float16, \"bf16\": torch.bfloat16}\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"gelu-1l\").to(DTYPES[cfg[\"enc_dtype\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa8000c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
