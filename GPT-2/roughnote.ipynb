{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9571d189",
   "metadata": {},
   "source": [
    "### Aligning Transformer Architecture to GPT2 State Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b2cd032",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc5cfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_gpt2_weights import convert_gpt2_weights, load_gpt2_weights, run_inference\n",
    "from gpt2 import TransformerSampler, ModelConfig, GenerationConfig\n",
    "\n",
    "model_cfg = ModelConfig()\n",
    "gen_cfg = GenerationConfig()\n",
    "sampler = load_gpt2_weights(model_cfg, gen_cfg)\n",
    "run_inference(sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6dff0b",
   "metadata": {},
   "source": [
    "# Train GPT 2 from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ac38e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/MechInter/GPT-2/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datasets\n",
    "from gpt2 import GPT2, ModelConfig, GenerationConfig\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class TrainingConfig():\n",
    "    batch_size = 1024\n",
    "    epochs = 1\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 1e-2\n",
    "    wandb_project: str | None = \"training_gpt2\"\n",
    "    wandb_name: str | None = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6eb2828",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model_cfg = ModelConfig()\n",
    "model_cfg.vocab_size = tokenizer.vocab_size\n",
    "\n",
    "gen_cfg = GenerationConfig()\n",
    "model = GPT2(model_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb2f6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 16/16 [00:00<00:00, 150131.69files/s]\n",
      "Generating train split: 0 examples [00:00, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 896668 examples [00:05, 175129.22 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset (format + tokenize)\n"
     ]
    }
   ],
   "source": [
    "story_ds = datasets.load_dataset(\"/home/ubuntu/MechInter/GPT-2/datasets/children-stories\", split=\"train\")\n",
    "\n",
    "\n",
    "def prepare_story_dataset(ds, tokenizer):\n",
    "\n",
    "    def format_and_tokenize(sample):\n",
    "        text = (\n",
    "            tokenizer.eos_token +\n",
    "            \"User: \" + sample[\"prompt\"] + tokenizer.eos_token + '\\n\\n'\n",
    "            \"Assistant: \" + sample[\"text\"] + tokenizer.eos_token\n",
    "        )\n",
    "\n",
    "        tokens = tokenizer(text, truncation=True, padding=False)\n",
    "        return {\"input_ids\": tokens['input_ids'], \"attention_mask\": tokens['attention_mask']}\n",
    "    \n",
    "    ds = ds.map(\n",
    "        format_and_tokenize, \n",
    "        num_proc=16,\n",
    "        remove_columns=ds.column_names,\n",
    "        desc=\"Formatting and tokenizing\",\n",
    "        cache_file_name=\"/home/ubuntu/MechInter/GPT-2/datasets/children-stories/cache.arrow\",\n",
    "        load_from_cache_file=True,\n",
    "        writer_batch_size=50000\n",
    "    )\n",
    "\n",
    "    return ds\n",
    "\n",
    "story_ds = prepare_story_dataset(story_ds, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1e093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, training_config: TrainingConfig, model: GPT2):\n",
    "        self.training_config = training_config\n",
    "        self.model = model\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=training_config.lr, weight_decay=training_config.weight_decay)\n",
    "        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=training_config.epochs)\n",
    "    \n",
    "    def train(self, dataset: datasets.Dataset):\n",
    "        dataloader = DataLoader(dataset, batch_size=self.training_config.batch_size, shuffle=True)\n",
    "        for epoch in range(self.training_config.epochs):\n",
    "            for batch in dataloader:\n",
    "                self.optimizer.zero_grad()\n",
    "                logits = self.model.forward(batch)\n",
    "                loss = self.compute_loss(logits, batch)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "    \n",
    "    def compute_loss(self, logits: torch.Tensor, batch: torch.Tensor):\n",
    "        loss = F.cross_entropy(logits, batch)\n",
    "        return loss\n",
    "    \n",
    "    def save_model(self, path: str):\n",
    "        torch.save(self.model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a842fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "redteaming_ds = datasets.load_dataset(\"/home/ubuntu/MechInter/GPT-2/datasets/redteaming-dataset\", split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7a30b8a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>\\n\\nUser:Write an'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([50256, 198, 198, 12982, 25, 16594, 281])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa90feaf",
   "metadata": {},
   "source": [
    "# Single layer transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b936fe8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gelu-1l into HookedTransformer\n",
      "Changing model dtype to torch.float32\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer, utils\n",
    "import torch\n",
    "cfg = {\n",
    "    \"seed\": 49,\n",
    "    \"batch_size\": 4096,\n",
    "    \"buffer_mult\": 384,\n",
    "    \"lr\": 1e-4,\n",
    "    \"num_tokens\": int(2e9),\n",
    "    \"l1_coeff\": 3e-4,\n",
    "    \"beta1\": 0.9,\n",
    "    \"beta2\": 0.99,\n",
    "    \"dict_mult\": 8,\n",
    "    \"seq_len\": 128,\n",
    "    \"d_mlp\": 2048,\n",
    "    \"enc_dtype\":\"fp32\",\n",
    "    \"remove_rare_dir\": False,\n",
    "}\n",
    "cfg[\"model_batch_size\"] = 64\n",
    "cfg[\"buffer_size\"] = cfg[\"batch_size\"] * cfg[\"buffer_mult\"]\n",
    "cfg[\"buffer_batches\"] = cfg[\"buffer_size\"] // cfg[\"seq_len\"]\n",
    "DTYPES = {\"fp32\": torch.float32, \"fp16\": torch.float16, \"bf16\": torch.bfloat16}\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"gelu-1l\").to(DTYPES[cfg[\"enc_dtype\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa8000c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
